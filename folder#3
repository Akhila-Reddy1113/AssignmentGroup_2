Intro to Machine Learning

Question:
UseHR.csvdata set and consider column ”left” to be the target variable with ”1” meaningthe
person left the company and ”0” meaning that the person did not leave the company.
–Investigate using various graphs/charts how given features affect this target variable.
–Choose several features and build thesklearnlogistic regression model predicting thetarget
variable ”left”.
–Discuss the model performance (the confusion matrix and theclassification report) onthe
test set.
import numpy as np
import pandas as pd
import matplotlib as plt
import seaborn as sns
from mlxtend.plotting import plot_decision_regions
data=pd.read_csv('HR.csv')
data
 satisfaction_level last_evaluation number_project \
0 0.38 0.53 2
1 0.80 0.86 5
2 0.11 0.88 7
3 0.72 0.87 5
4 0.37 0.52 2
... ... ... ...
14994 0.40 0.57 2
14995 0.37 0.48 2
14996 0.37 0.53 2
14997 0.11 0.96 6
14998 0.37 0.52 2
 average_montly_hours time_spend_company Work_accident
left \
0 157 3 0 1
1 262 6 0 1
2 272 4 0 1
3 223 5 0 1
4 159 3 0 1
... ... ... ... ...
14994 151 3 0 1
14995 160 3 0 1
14996 143 3 0 1
14997 280 4 0 1
14998 158 3 0 1
 promotion_last_5years Department salary
0 0 sales low
1 0 sales medium
2 0 sales medium
3 0 sales low
4 0 sales low
... ... ... ...
14994 0 support low
14995 0 support low
14996 0 support low
14997 0 support low
14998 0 support low
[14999 rows x 10 columns]
data=pd.get_dummies(data,columns=['Department','salary'])
data
 satisfaction_level last_evaluation number_project \
0 0.38 0.53 2
1 0.80 0.86 5
2 0.11 0.88 7
3 0.72 0.87 5
4 0.37 0.52 2
... ... ... ...
14994 0.40 0.57 2
14995 0.37 0.48 2
14996 0.37 0.53 2
14997 0.11 0.96 6
14998 0.37 0.52 2
 average_montly_hours time_spend_company Work_accident
left \
0 157 3 0 1
1 262 6 0 1
2 272 4 0 1
3 223 5 0 1
4 159 3 0 1
... ... ... ... ...
14994 151 3 0 1
14995 160 3 0 1
14996 143 3 0 1
14997 280 4 0 1
14998 158 3 0 1
 promotion_last_5years Department_IT Department_RandD ... \
0 0 0 0 ...
1 0 0 0 ...
2 0 0 0 ...
3 0 0 0 ...
4 0 0 0 ...
... ... ... ... ...
14994 0 0 0 ...
14995 0 0 0 ...
14996 0 0 0 ...
14997 0 0 0 ...
14998 0 0 0 ...
 Department_hr Department_management Department_marketing \
0 0 0 0
1 0 0 0
2 0 0 0
3 0 0 0
4 0 0 0
... ... ... ...
14994 0 0 0
14995 0 0 0
14996 0 0 0
14997 0 0 0
14998 0 0 0
 Department_product_mng Department_sales Department_support \
0 0 1 0
1 0 1 0
2 0 1 0
3 0 1 0
4 0 1 0
... ... ... ...
14994 0 0 1
14995 0 0 1
14996 0 0 1
14997 0 0 1
14998 0 0 1
 Department_technical salary_high salary_low salary_medium
0 0 0 1 0
1 0 0 0 1
2 0 0 0 1
3 0 0 1 0
4 0 0 1 0
... ... ... ... ...
14994 0 0 1 0
14995 0 0 1 0
14996 0 0 1 0
14997 0 0 1 0
14998 0 0 1 0
[14999 rows x 21 columns]
from sklearn.preprocessing import PolynomialFeatures
# perform a polynomial features transform of the dataset
trans = PolynomialFeatures(degree=2)
dataset =
trans.fit_transform(X=data[["satisfaction_level","salary_high","salary
_low","Work_accident","time_spend_company"]])
dataset = pd.DataFrame(dataset)
dataset=pd.concat([dataset, data.left], axis=1)
dataset.drop(0,axis=1)
dataset
 0 1 2 3 4 5 6 7 8 9 ... 12
13 \
0 1.0 0.38 0.0 1.0 0.0 3.0 0.1444 0.0 0.38 0.0 ... 0.0
0.0
1 1.0 0.80 0.0 0.0 0.0 6.0 0.6400 0.0 0.00 0.0 ... 0.0
0.0
2 1.0 0.11 0.0 0.0 0.0 4.0 0.0121 0.0 0.00 0.0 ... 0.0
0.0
3 1.0 0.72 0.0 1.0 0.0 5.0 0.5184 0.0 0.72 0.0 ... 0.0
0.0
4 1.0 0.37 0.0 1.0 0.0 3.0 0.1369 0.0 0.37 0.0 ... 0.0
0.0
... ... ... ... ... ... ... ... ... ... ... ... ...
...
14994 1.0 0.40 0.0 1.0 0.0 3.0 0.1600 0.0 0.40 0.0 ... 0.0
0.0
14995 1.0 0.37 0.0 1.0 0.0 3.0 0.1369 0.0 0.37 0.0 ... 0.0
0.0
14996 1.0 0.37 0.0 1.0 0.0 3.0 0.1369 0.0 0.37 0.0 ... 0.0
0.0
14997 1.0 0.11 0.0 1.0 0.0 4.0 0.0121 0.0 0.11 0.0 ... 0.0
0.0
14998 1.0 0.37 0.0 1.0 0.0 3.0 0.1369 0.0 0.37 0.0 ... 0.0
0.0
 14 15 16 17 18 19 20 left
0 0.0 1.0 0.0 3.0 0.0 0.0 9.0 1
1 0.0 0.0 0.0 0.0 0.0 0.0 36.0 1
2 0.0 0.0 0.0 0.0 0.0 0.0 16.0 1
3 0.0 1.0 0.0 5.0 0.0 0.0 25.0 1
4 0.0 1.0 0.0 3.0 0.0 0.0 9.0 1
... ... ... ... ... ... ... ... ...
14994 0.0 1.0 0.0 3.0 0.0 0.0 9.0 1
14995 0.0 1.0 0.0 3.0 0.0 0.0 9.0 1
14996 0.0 1.0 0.0 3.0 0.0 0.0 9.0 1
14997 0.0 1.0 0.0 4.0 0.0 0.0 16.0 1
14998 0.0 1.0 0.0 3.0 0.0 0.0 9.0 1
[14999 rows x 22 columns]
dataset.drop(0,axis=1)
dataset
 0 1 2 3 4 5 6 7 8 9 ... 12
13 \
0 1.0 0.38 0.0 1.0 0.0 3.0 0.1444 0.0 0.38 0.0 ... 0.0
0.0
1 1.0 0.80 0.0 0.0 0.0 6.0 0.6400 0.0 0.00 0.0 ... 0.0
0.0
2 1.0 0.11 0.0 0.0 0.0 4.0 0.0121 0.0 0.00 0.0 ... 0.0
0.0
3 1.0 0.72 0.0 1.0 0.0 5.0 0.5184 0.0 0.72 0.0 ... 0.0
0.0
4 1.0 0.37 0.0 1.0 0.0 3.0 0.1369 0.0 0.37 0.0 ... 0.0
0.0
... ... ... ... ... ... ... ... ... ... ... ... ...
...
14994 1.0 0.40 0.0 1.0 0.0 3.0 0.1600 0.0 0.40 0.0 ... 0.0
0.0
14995 1.0 0.37 0.0 1.0 0.0 3.0 0.1369 0.0 0.37 0.0 ... 0.0
0.0
14996 1.0 0.37 0.0 1.0 0.0 3.0 0.1369 0.0 0.37 0.0 ... 0.0
0.0
14997 1.0 0.11 0.0 1.0 0.0 4.0 0.0121 0.0 0.11 0.0 ... 0.0
0.0
14998 1.0 0.37 0.0 1.0 0.0 3.0 0.1369 0.0 0.37 0.0 ... 0.0
0.0
 14 15 16 17 18 19 20 left
0 0.0 1.0 0.0 3.0 0.0 0.0 9.0 1
1 0.0 0.0 0.0 0.0 0.0 0.0 36.0 1
2 0.0 0.0 0.0 0.0 0.0 0.0 16.0 1
3 0.0 1.0 0.0 5.0 0.0 0.0 25.0 1
4 0.0 1.0 0.0 3.0 0.0 0.0 9.0 1
... ... ... ... ... ... ... ... ...
14994 0.0 1.0 0.0 3.0 0.0 0.0 9.0 1
14995 0.0 1.0 0.0 3.0 0.0 0.0 9.0 1
14996 0.0 1.0 0.0 3.0 0.0 0.0 9.0 1
14997 0.0 1.0 0.0 4.0 0.0 0.0 16.0 1
14998 0.0 1.0 0.0 3.0 0.0 0.0 9.0 1
[14999 rows x 22 columns]
min(data.left)
0
Visualizations and EDA to get an idea on how the label is effected by other
features
dataset.describe()
 0 1 2 3 4
\
count 14999.0 14999.000000 14999.000000 14999.000000 14999.000000
mean 1.0 0.612834 0.082472 0.487766 0.144610
std 0.0 0.248631 0.275092 0.499867 0.351719
min 1.0 0.090000 0.000000 0.000000 0.000000
25% 1.0 0.440000 0.000000 0.000000 0.000000
50% 1.0 0.640000 0.000000 0.000000 0.000000
75% 1.0 0.820000 0.000000 1.000000 0.000000
max 1.0 1.000000 1.000000 1.000000 1.000000
 5 6 7 8
9 \
count 14999.000000 14999.000000 14999.000000 14999.000000
14999.000000
mean 3.498233 0.437378 0.052574 0.293027
0.093754
std 1.460136 0.282840 0.187058 0.348987
0.244383
min 2.000000 0.008100 0.000000 0.000000
0.000000
25% 3.000000 0.193600 0.000000 0.000000
0.000000
50% 3.000000 0.409600 0.000000 0.000000
0.000000
75% 4.000000 0.672400 0.000000 0.620000
0.000000
max 10.000000 1.000000 1.000000 1.000000
1.000000
 ... 12 13 14 15
16 \
count ... 14999.0 14999.000000 14999.000000 14999.000000
14999.000000
mean ... 0.0 0.012801 0.304554 0.487766
0.069338
std ... 0.0 0.112418 1.163773 0.499867
0.254036
min ... 0.0 0.000000 0.000000 0.000000
0.000000
25% ... 0.0 0.000000 0.000000 0.000000
0.000000
50% ... 0.0 0.000000 0.000000 0.000000
0.000000
75% ... 0.0 0.000000 0.000000 1.000000
0.000000
max ... 0.0 1.000000 10.000000 1.000000
1.000000
 17 18 19 20
left
count 14999.000000 14999.000000 14999.000000 14999.000000
14999.000000
mean 1.677045 0.144610 0.506967 14.369491
0.238083
std 1.944962 0.351719 1.384217 14.692120
0.425924
min 0.000000 0.000000 0.000000 4.000000
0.000000
25% 0.000000 0.000000 0.000000 9.000000
0.000000
50% 0.000000 0.000000 0.000000 9.000000
0.000000
75% 3.000000 0.000000 0.000000 16.000000
0.000000
max 10.000000 1.000000 10.000000 100.000000
1.000000
[8 rows x 22 columns]
dataset.corr()
 0 1 2 3 4 5
6 \
0 NaN NaN NaN NaN NaN NaN NaN
1 NaN 1.000000 0.029708 -0.047415 0.058697 -0.100866 0.973324
2 NaN 0.029708 1.000000 -0.292560 0.009040 0.039953 0.021606
3 NaN -0.047415 -0.292560 1.000000 -0.006813 -0.040110 -0.040245
4 NaN 0.058697 0.009040 -0.006813 1.000000 0.002120 0.052876
5 NaN -0.100866 0.039953 -0.040110 0.002120 1.000000 -0.065012
6 NaN 0.973324 0.021606 -0.040245 0.052876 -0.065012 1.000000
7 NaN 0.118990 0.937476 -0.274268 0.012007 0.019390 0.112980
8 NaN 0.323546 -0.251743 0.860481 0.012838 -0.065366 0.316001
9 NaN 0.181924 0.009825 -0.014571 0.933078 -0.014310 0.179144
10 NaN 0.646988 0.042575 -0.054781 0.039175 0.651001 0.638080
11 NaN 0.029708 1.000000 -0.292560 0.009040 0.039953 0.021606
12 NaN NaN NaN NaN NaN NaN NaN
13 NaN 0.019599 0.379816 -0.111119 0.276950 0.014761 0.017201
14 NaN 0.008880 0.872903 -0.255377 0.007725 0.224600 0.004490
15 NaN -0.047415 -0.292560 1.000000 -0.006813 -0.040110 -0.040245
16 NaN 0.023080 -0.081834 0.279716 0.663855 -0.018724 0.020780
17 NaN -0.074407 -0.258519 0.883643 -0.010089 0.256508 -0.054168
18 NaN 0.058697 0.009040 -0.006813 1.000000 0.002120 0.052876
19 NaN 0.035398 0.014162 -0.016861 0.890787 0.197651 0.036013
20 NaN -0.050056 0.064816 -0.056354 0.018381 0.964227 -0.026555
left NaN -0.388375 -0.120929 0.134722 -0.154622 0.144822 -0.344000
 7 8 9 ... 12 13 14
15 \
0 NaN NaN NaN ... NaN NaN NaN
NaN
1 0.118990 0.323546 0.181924 ... NaN 0.019599 0.008880 -
0.047415
2 0.937476 -0.251743 0.009825 ... NaN 0.379816 0.872903 -
0.292560
3 -0.274268 0.860481 -0.014571 ... NaN -0.111119 -0.255377
1.000000
4 0.012007 0.012838 0.933078 ... NaN 0.276950 0.007725 -
0.006813
5 0.019390 -0.065366 -0.014310 ... NaN 0.014761 0.224600 -
0.040110
6 0.112980 0.316001 0.179144 ... NaN 0.017201 0.004490 -
0.040245
7 1.000000 -0.236003 0.026618 ... NaN 0.367121 0.795660 -
0.274268
8 -0.236003 1.000000 0.050977 ... NaN -0.095616 -0.219747
0.860481
9 0.026618 0.050977 1.000000 ... NaN 0.261817 0.005465 -
0.014571
10 0.097021 0.187646 0.118933 ... NaN 0.021165 0.155715 -
0.054781
11 0.937476 -0.251743 0.009825 ... NaN 0.379816 0.872903 -
0.292560
12 NaN NaN NaN ... NaN NaN NaN
NaN
13 0.367121 -0.095616 0.261817 ... NaN 1.000000 0.331023 -
0.111119
14 0.795660 -0.219747 0.005465 ... NaN 0.331023 1.000000 -
0.255377
15 -0.274268 0.860481 -0.014571 ... NaN -0.111119 -0.255377
1.000000
16 -0.076717 0.266582 0.603266 ... NaN -0.031082 -0.071433
0.279716
17 -0.242355 0.737197 -0.025537 ... NaN -0.098190 -0.225662
0.883643
18 0.012007 0.012838 0.933078 ... NaN 0.276950 0.007725 -
0.006813
19 0.013342 -0.007167 0.813991 ... NaN 0.261654 0.048351 -
0.016861
20 0.045464 -0.061399 0.008781 ... NaN 0.029270 0.266246 -
0.056354
left -0.127296 -0.039478 -0.166064 ... NaN -0.063654 -0.105669
0.134722
 16 17 18 19 20 left
0 NaN NaN NaN NaN NaN NaN
1 0.023080 -0.074407 0.058697 0.035398 -0.050056 -0.388375
2 -0.081834 -0.258519 0.009040 0.014162 0.064816 -0.120929
3 0.279716 0.883643 -0.006813 -0.016861 -0.056354 0.134722
4 0.663855 -0.010089 1.000000 0.890787 0.018381 -0.154622
5 -0.018724 0.256508 0.002120 0.197651 0.964227 0.144822
6 0.020780 -0.054168 0.052876 0.036013 -0.026555 -0.344000
7 -0.076717 -0.242355 0.012007 0.013342 0.045464 -0.127296
8 0.266582 0.737197 0.012838 -0.007167 -0.061399 -0.039478
9 0.603266 -0.025537 0.933078 0.813991 0.008781 -0.166064
10 -0.003041 0.141321 0.039175 0.170308 0.660808 -0.121405
11 -0.081834 -0.258519 0.009040 0.014162 0.064816 -0.120929
12 NaN NaN NaN NaN NaN NaN
13 -0.031082 -0.098190 0.276950 0.261654 0.029270 -0.063654
14 -0.071433 -0.225662 0.007725 0.048351 0.266246 -0.105669
15 0.279716 0.883643 -0.006813 -0.016861 -0.056354 0.134722
16 1.000000 0.241536 0.663855 0.570118 -0.009544 -0.094039
17 0.241536 1.000000 -0.010089 0.041055 0.213198 0.195363
18 0.663855 -0.010089 1.000000 0.890787 0.018381 -0.154622
19 0.570118 0.041055 0.890787 1.000000 0.215230 -0.130211
20 -0.009544 0.213198 0.018381 0.215230 1.000000 0.061389
left -0.094039 0.195363 -0.154622 -0.130211 0.061389 1.000000
[22 rows x 22 columns]
sns.set(rc={'figure.figsize':(15,8)})
sns.heatmap(data=data.corr(), annot=True , cmap="coolwarm");
sns.pairplot(data[["left","satisfaction_level","salary_high","salary_l
ow","Work_accident","time_spend_company"]])
<seaborn.axisgrid.PairGrid at 0x284b92ba9d0>
add additional visualizations but i have used satisfaction_level for
model
X=data[['satisfaction_level']]
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, data.left,
test_size=0.2)
len(X_train)
11999
len(X_test)
3000
model = LogisticRegression(max_iter=10_000)
model.fit(X_train, y_train)
LogisticRegression(max_iter=10000)
model.score(X_train, y_train)
0.7693141095091257
model.score(X_test, y_test)
0.7733333333333333
X=dataset.drop("left",axis=1)
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, dataset.left,
test_size=0.2)
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
# define the multinomial logistic regression model
model = LogisticRegression(multi_class='multinomial', solver='lbfgs')
# define the model evaluation procedure
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
# evaluate the model and collect the scores
n_scores = cross_val_score(model, X_train, y_train,
scoring='accuracy', cv=cv, n_jobs=-1)
# report the model performance
# evaluate multinomial logistic regression model
from numpy import mean
from numpy import std
print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))
Mean Accuracy: 0.839 (0.013)
